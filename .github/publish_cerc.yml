# .github/workflows/schedule_scraper.yml

name: Scheduled Cache Update

on:
  workflow_dispatch:  # Allows manual triggering from the GitHub Actions tab
  schedule:
    # Runs at 04:00 UTC every day (approx 9:30 AM IST)
    - cron: '0 4 * * *'

jobs:
  scrape_and_commit:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        # Need to fetch history to commit the new file
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11' # Change to your required version
          cache: 'pip'

      - name: Install dependencies
        # Ensure 'requests' and 'beautifulsoup4' are in your requirements.txt
        run: pip install -r requirements.txt

      - name: Run Scraper and Update Cache
        # This calls the update_cache.py script
        run: python update_cache.py

      - name: Commit and Push new Cache File
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          # Only commit the changes in the data/cache directory
          file_pattern: 'data/cache/*.json'
          commit_message: 'ðŸ¤– AUTO: Update CTUIL data cache from scheduled scrape'
          branch: main # or master